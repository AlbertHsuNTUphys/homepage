{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a1ee40",
   "metadata": {},
   "source": [
    "# Creation of CaloChallenge 2022 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54aecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1f507",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466dcf8",
   "metadata": {},
   "source": [
    "### Photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210faa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Based on high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv, take first N events (size of actual GAN training sets)\n",
    "        - check for nans\n",
    "        - create dataset in hdf5 file of that energy\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/photons_samples_highStat/'\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "              16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 10000, \n",
    "              524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "dataset_file = h5py.File(folder+'dataset_1_photons.hdf5', 'w')\n",
    "\n",
    "for n in range(8,23):\n",
    "    energy = float(2**n)\n",
    "    file_name = folder+'pid22_E'+str(2**n)+'_eta_20_25_voxalisation.csv'\n",
    "    loaded_array = pd.read_csv(file_name, header=None).to_numpy()\n",
    "    if np.isnan(loaded_array[:num_events[energy]]).any():\n",
    "        raise ValueError(\"Dataset contains NaNs!\")\n",
    "    dataset_file.create_dataset('data_'+str(int(energy)), data=loaded_array[:num_events[energy]].clip(min=0.),\n",
    "                               compression='gzip')\n",
    "\n",
    "dataset_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094c0f6",
   "metadata": {},
   "source": [
    "### Pions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091cc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Based on high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv, take all given events until high-stats are ready (only events at 4 TeV are missing)\n",
    "        - check for nans\n",
    "        - create dataset in hdf5 file of that energy\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/pion_samples/'\n",
    "\n",
    "# not used for now:\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "             16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 10000, \n",
    "             524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "dataset_file = h5py.File(folder+'dataset_1_pions.hdf5', 'w')\n",
    "\n",
    "for n in range(8,23):\n",
    "    energy = float(2**n)\n",
    "    file_name = folder+'pid211_E'+str(2**n)+'_eta_20_25_voxalisation.csv'\n",
    "    loaded_array = pd.read_csv(file_name, header=None).to_numpy()\n",
    "    if np.isnan(loaded_array).any():\n",
    "        raise ValueError(\"Dataset contains NaNs!\")\n",
    "    dataset_file.create_dataset('data_'+str(int(energy)), data=loaded_array.clip(min=0.), compression='gzip')\n",
    "\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d5006",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b232f",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b55da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with reading file 1/120\n",
      "Done with reading file 2/120\n",
      "Done with reading file 3/120\n",
      "Done with reading file 4/120\n",
      "Done with reading file 5/120\n",
      "Done with reading file 6/120\n",
      "Done with reading file 7/120\n",
      "Done with reading file 8/120\n",
      "Done with reading file 9/120\n",
      "Done with reading file 10/120\n",
      "Done with reading file 11/120\n",
      "Done with reading file 12/120\n",
      "Done with reading file 13/120\n",
      "Done with reading file 14/120\n",
      "Done with reading file 15/120\n",
      "Done with reading file 16/120\n",
      "Done with reading file 17/120\n",
      "Done with reading file 18/120\n",
      "Done with reading file 19/120\n",
      "Done with reading file 20/120\n",
      "Done with reading file 21/120\n",
      "Done with reading file 22/120\n",
      "Done with reading file 23/120\n",
      "Done with reading file 24/120\n",
      "Done with reading file 25/120\n",
      "Done with reading file 26/120\n",
      "Done with reading file 27/120\n",
      "Done with reading file 28/120\n",
      "Done with reading file 29/120\n",
      "Done with reading file 30/120\n",
      "Done with reading file 31/120\n",
      "Done with reading file 32/120\n",
      "Done with reading file 33/120\n",
      "Done with reading file 34/120\n",
      "Done with reading file 35/120\n",
      "Done with reading file 36/120\n",
      "Done with reading file 37/120\n",
      "Done with reading file 38/120\n",
      "Done with reading file 39/120\n",
      "Done with reading file 40/120\n",
      "Done with reading file 41/120\n",
      "Done with reading file 42/120\n",
      "Done with reading file 43/120\n",
      "Done with reading file 44/120\n",
      "Done with reading file 45/120\n",
      "Done with reading file 46/120\n",
      "Done with reading file 47/120\n",
      "Done with reading file 48/120\n",
      "Done with reading file 49/120\n",
      "Done with reading file 50/120\n",
      "Done with reading file 51/120\n",
      "Done with reading file 52/120\n",
      "Done with reading file 53/120\n",
      "Done with reading file 54/120\n",
      "Done with reading file 55/120\n",
      "Done with reading file 56/120\n",
      "Done with reading file 57/120\n",
      "Done with reading file 58/120\n",
      "Done with reading file 59/120\n",
      "Done with reading file 60/120\n",
      "150000\n",
      "(150000, 9, 16, 45)\n",
      "(150000, 45, 9, 16)\n",
      "(150000, 45, 16, 9)\n",
      "Done with writing file 1\n",
      "Done with reading file 61/120\n",
      "Done with reading file 62/120\n",
      "Done with reading file 63/120\n",
      "Done with reading file 64/120\n",
      "Done with reading file 65/120\n",
      "Done with reading file 66/120\n",
      "Done with reading file 67/120\n",
      "Done with reading file 68/120\n",
      "Done with reading file 69/120\n",
      "Done with reading file 70/120\n",
      "Done with reading file 71/120\n",
      "Done with reading file 72/120\n",
      "Done with reading file 73/120\n",
      "Done with reading file 74/120\n",
      "Done with reading file 75/120\n",
      "Done with reading file 76/120\n",
      "Done with reading file 77/120\n",
      "Done with reading file 78/120\n",
      "Done with reading file 79/120\n",
      "Done with reading file 80/120\n",
      "Done with reading file 81/120\n",
      "Done with reading file 82/120\n",
      "Done with reading file 83/120\n",
      "Done with reading file 84/120\n",
      "Done with reading file 85/120\n",
      "Done with reading file 86/120\n",
      "Done with reading file 87/120\n",
      "Done with reading file 88/120\n",
      "Done with reading file 89/120\n",
      "Done with reading file 90/120\n",
      "Done with reading file 91/120\n",
      "Done with reading file 92/120\n",
      "Done with reading file 93/120\n",
      "Done with reading file 94/120\n",
      "Done with reading file 95/120\n",
      "Done with reading file 96/120\n",
      "Done with reading file 97/120\n",
      "Done with reading file 98/120\n",
      "Done with reading file 99/120\n",
      "Done with reading file 100/120\n",
      "Done with reading file 101/120\n",
      "Done with reading file 102/120\n",
      "Done with reading file 103/120\n",
      "Done with reading file 104/120\n",
      "Done with reading file 105/120\n",
      "Done with reading file 106/120\n",
      "Done with reading file 107/120\n",
      "Done with reading file 108/120\n",
      "Done with reading file 109/120\n",
      "Done with reading file 110/120\n",
      "Done with reading file 111/120\n",
      "Done with reading file 112/120\n",
      "Done with reading file 113/120\n",
      "Done with reading file 114/120\n",
      "Done with reading file 115/120\n",
      "Done with reading file 116/120\n",
      "Done with reading file 117/120\n",
      "Done with reading file 118/120\n",
      "Done with reading file 119/120\n",
      "Done with reading file 120/120\n",
      "150000\n",
      "(150000, 9, 16, 45)\n",
      "(150000, 45, 9, 16)\n",
      "(150000, 45, 16, 9)\n",
      "Done with writing file 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on Dataset2_GPS of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA\n",
    "    steps:\n",
    "        - load existing hdf5 files\n",
    "        - read out energy and shower\n",
    "        - concatenate to list of 150k showers\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - rescale by sampling_fraction, as given by Anna\n",
    "        - write to new hdf5 files\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset2_cont_energy/'\n",
    "sampling_fraction = 1./0.033\n",
    "\n",
    "energy = []\n",
    "shower = []\n",
    "\n",
    "output_nr = 1\n",
    "\n",
    "for idx, source_file in enumerate(glob.glob(folder+'*')):\n",
    "    data_source = h5py.File(source_file, 'r')\n",
    "\n",
    "    for key in data_source[\"Angle_90\"].keys():\n",
    "        energy.append(float(key))\n",
    "        shower.append(data_source[\"Angle_90\"][key][:])\n",
    "    data_source.close()\n",
    "    print(\"Done with reading file {}/{}\".format(idx+1, 120))\n",
    "    if idx % 60 == 59:\n",
    "        energy = np.array(energy)\n",
    "        print(len(energy))\n",
    "        shower = np.array(shower)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 1)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 2)\n",
    "        print(shower.shape)\n",
    "\n",
    "        dataset_file = h5py.File(folder + 'dataset_2_{}.hdf5'.format(output_nr), 'w')\n",
    "        dataset_file.create_dataset('incident_energies', data=energy.clip(min=0.).reshape(len(energy), -1), compression='gzip')\n",
    "        dataset_file.create_dataset('showers', data=sampling_fraction*shower.clip(min=0.).reshape(len(shower), -1), compression='gzip')\n",
    "        print(\"Done with writing file {}\".format(output_nr))\n",
    "        dataset_file.close()\n",
    "        output_nr += 1\n",
    "        energy = []\n",
    "        shower = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b44abb",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eda4fd",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa98af7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with reading file 1/120\n",
      "Done with reading file 2/120\n",
      "Done with reading file 3/120\n",
      "Done with reading file 4/120\n",
      "Done with reading file 5/120\n",
      "Done with reading file 6/120\n",
      "Done with reading file 7/120\n",
      "Done with reading file 8/120\n",
      "Done with reading file 9/120\n",
      "Done with reading file 10/120\n",
      "Done with reading file 11/120\n",
      "Done with reading file 12/120\n",
      "Done with reading file 13/120\n",
      "Done with reading file 14/120\n",
      "Done with reading file 15/120\n",
      "Done with reading file 16/120\n",
      "Done with reading file 17/120\n",
      "Done with reading file 18/120\n",
      "Done with reading file 19/120\n",
      "Done with reading file 20/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 1\n",
      "Done with reading file 21/120\n",
      "Done with reading file 22/120\n",
      "Done with reading file 23/120\n",
      "Done with reading file 24/120\n",
      "Done with reading file 25/120\n",
      "Done with reading file 26/120\n",
      "Done with reading file 27/120\n",
      "Done with reading file 28/120\n",
      "Done with reading file 29/120\n",
      "Done with reading file 30/120\n",
      "Done with reading file 31/120\n",
      "Done with reading file 32/120\n",
      "Done with reading file 33/120\n",
      "Done with reading file 34/120\n",
      "Done with reading file 35/120\n",
      "Done with reading file 36/120\n",
      "Done with reading file 37/120\n",
      "Done with reading file 38/120\n",
      "Done with reading file 39/120\n",
      "Done with reading file 40/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 2\n",
      "Done with reading file 41/120\n",
      "Done with reading file 42/120\n",
      "Done with reading file 43/120\n",
      "Done with reading file 44/120\n",
      "Done with reading file 45/120\n",
      "Done with reading file 46/120\n",
      "Done with reading file 47/120\n",
      "Done with reading file 48/120\n",
      "Done with reading file 49/120\n",
      "Done with reading file 50/120\n",
      "Done with reading file 51/120\n",
      "Done with reading file 52/120\n",
      "Done with reading file 53/120\n",
      "Done with reading file 54/120\n",
      "Done with reading file 55/120\n",
      "Done with reading file 56/120\n",
      "Done with reading file 57/120\n",
      "Done with reading file 58/120\n",
      "Done with reading file 59/120\n",
      "Done with reading file 60/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 3\n",
      "Done with reading file 61/120\n",
      "Done with reading file 62/120\n",
      "Done with reading file 63/120\n",
      "Done with reading file 64/120\n",
      "Done with reading file 65/120\n",
      "Done with reading file 66/120\n",
      "Done with reading file 67/120\n",
      "Done with reading file 68/120\n",
      "Done with reading file 69/120\n",
      "Done with reading file 70/120\n",
      "Done with reading file 71/120\n",
      "Done with reading file 72/120\n",
      "Done with reading file 73/120\n",
      "Done with reading file 74/120\n",
      "Done with reading file 75/120\n",
      "Done with reading file 76/120\n",
      "Done with reading file 77/120\n",
      "Done with reading file 78/120\n",
      "Done with reading file 79/120\n",
      "Done with reading file 80/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 4\n",
      "Done with reading file 81/120\n",
      "Done with reading file 82/120\n",
      "Done with reading file 83/120\n",
      "Done with reading file 84/120\n",
      "Done with reading file 85/120\n",
      "Done with reading file 86/120\n",
      "Done with reading file 87/120\n",
      "Done with reading file 88/120\n",
      "Done with reading file 89/120\n",
      "Done with reading file 90/120\n",
      "Done with reading file 91/120\n",
      "Done with reading file 92/120\n",
      "Done with reading file 93/120\n",
      "Done with reading file 94/120\n",
      "Done with reading file 95/120\n",
      "Done with reading file 96/120\n",
      "Done with reading file 97/120\n",
      "Done with reading file 98/120\n",
      "Done with reading file 99/120\n",
      "Done with reading file 100/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 5\n",
      "Done with reading file 101/120\n",
      "Done with reading file 102/120\n",
      "Done with reading file 103/120\n",
      "Done with reading file 104/120\n",
      "Done with reading file 105/120\n",
      "Done with reading file 106/120\n",
      "Done with reading file 107/120\n",
      "Done with reading file 108/120\n",
      "Done with reading file 109/120\n",
      "Done with reading file 110/120\n",
      "Done with reading file 111/120\n",
      "Done with reading file 112/120\n",
      "Done with reading file 113/120\n",
      "Done with reading file 114/120\n",
      "Done with reading file 115/120\n",
      "Done with reading file 116/120\n",
      "Done with reading file 117/120\n",
      "Done with reading file 118/120\n",
      "Done with reading file 119/120\n",
      "Done with reading file 120/120\n",
      "50000\n",
      "(50000, 18, 50, 45)\n",
      "(50000, 45, 18, 50)\n",
      "(50000, 45, 50, 18)\n",
      "Done with writing file 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on Dataset3_GPS of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA\n",
    "    steps:\n",
    "        - load existing hdf5 files\n",
    "        - read out energy and shower\n",
    "        - concatenate to list of 50k showers\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - rescale by sampling_fraction, as given by Anna\n",
    "        - write to new hdf5 files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset3_cont_energy/'\n",
    "sampling_fraction = 1./0.033\n",
    "\n",
    "energy = []\n",
    "shower = []\n",
    "\n",
    "output_nr = 1\n",
    "\n",
    "for idx, source_file in enumerate(glob.glob(folder+'*')):\n",
    "    data_source = h5py.File(source_file, 'r')\n",
    "\n",
    "    for key in data_source[\"Angle_90\"].keys():\n",
    "        energy.append(float(key))\n",
    "        shower.append(data_source[\"Angle_90\"][key][:])\n",
    "    data_source.close()\n",
    "    print(\"Done with reading file {}/{}\".format(idx+1, 120))\n",
    "    if idx % 20 == 19:\n",
    "        energy = np.array(energy)\n",
    "        print(len(energy))\n",
    "        shower = np.array(shower)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 1)\n",
    "        print(shower.shape)\n",
    "        shower = np.moveaxis(shower, 3, 2)\n",
    "        print(shower.shape)\n",
    "\n",
    "        dataset_file = h5py.File(folder + 'dataset_3_{}.hdf5'.format(output_nr), 'w')\n",
    "        dataset_file.create_dataset('incident_energies', data=energy.clip(min=0.).reshape(len(energy), -1), compression='gzip')\n",
    "        dataset_file.create_dataset('showers', data=sampling_fraction*shower.clip(min=0.).reshape(len(shower), -1), compression='gzip')\n",
    "        print(\"Done with writing file {}\".format(output_nr))\n",
    "        dataset_file.close()\n",
    "        output_nr += 1\n",
    "        energy = []\n",
    "        shower = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7c6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
