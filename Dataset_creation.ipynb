{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a1ee40",
   "metadata": {},
   "source": [
    "# Creation of CaloChallenge 2022 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54aecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1f507",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466dcf8",
   "metadata": {},
   "source": [
    "### Photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210faa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Based on high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv, take first N events (size of actual GAN training sets)\n",
    "        - check for nans\n",
    "        - create dataset in hdf5 file of that energy\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/photons_samples_highStat/'\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "              16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 10000, \n",
    "              524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "dataset_file = h5py.File(folder+'dataset_1_photons.hdf5', 'w')\n",
    "\n",
    "for n in range(8,23):\n",
    "    energy = float(2**n)\n",
    "    file_name = folder+'pid22_E'+str(2**n)+'_eta_20_25_voxalisation.csv'\n",
    "    loaded_array = pd.read_csv(file_name, header=None).to_numpy()\n",
    "    if np.isnan(loaded_array[:num_events[energy]]).any():\n",
    "        raise ValueError(\"Dataset contains NaNs!\")\n",
    "    dataset_file.create_dataset('data_'+str(int(energy)), data=loaded_array[:num_events[energy]].clip(min=0.),\n",
    "                               compression='gzip')\n",
    "\n",
    "dataset_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094c0f6",
   "metadata": {},
   "source": [
    "### Pions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091cc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Based on high-stats data of https://opendata.cern.ch/record/15012# \n",
    "    steps:\n",
    "        - load csv, take all given events until high-stats are ready (only events at 4 TeV are missing)\n",
    "        - check for nans\n",
    "        - create dataset in hdf5 file of that energy\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/pion_samples/'\n",
    "\n",
    "# not used for now:\n",
    "num_events = {256: 10000, 512: 10000, 1024: 10000, 2048: 10000, 4096: 10000, 8192: 10000,\n",
    "             16384: 10000, 32768: 10000, 65536: 10000, 131072: 10000, 262144: 10000, \n",
    "             524288: 5000, 1048576: 3000, 2097152: 2000, 4194304: 1000}\n",
    "\n",
    "dataset_file = h5py.File(folder+'dataset_1_pions.hdf5', 'w')\n",
    "\n",
    "for n in range(8,23):\n",
    "    energy = float(2**n)\n",
    "    file_name = folder+'pid211_E'+str(2**n)+'_eta_20_25_voxalisation.csv'\n",
    "    loaded_array = pd.read_csv(file_name, header=None).to_numpy()\n",
    "    if np.isnan(loaded_array).any():\n",
    "        raise ValueError(\"Dataset contains NaNs!\")\n",
    "    dataset_file.create_dataset('data_'+str(int(energy)), data=loaded_array.clip(min=0.), compression='gzip')\n",
    "\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d5006",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b232f",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebd33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Based on dataset 2 of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA\n",
    "    steps:\n",
    "        - load existing hdf5 file\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - write to new hdf5 file\n",
    "\n",
    "\"\"\"\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset2/'\n",
    "\n",
    "data_source = h5py.File(folder+'SiW_LowGran.hdf5', 'r')\n",
    "dataset_file = h5py.File(folder + 'dataset_2.hdf5', 'w')\n",
    "\n",
    "for n in range(0, 11):\n",
    "    energy = 2**n\n",
    "    data = data_source[str(2**n)][\"data\"][:]\n",
    "    data = np.moveaxis(data, 3, 1)\n",
    "    data = np.moveaxis(data, 3, 2)\n",
    "    dataset_file.create_dataset('data_'+str(energy*1000), data=data.clip(min=0.).reshape(len(data), -1),\n",
    "                               compression='gzip')\n",
    "\n",
    "data_source.close()\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b44abb",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eda4fd",
   "metadata": {},
   "source": [
    "### Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed980140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with angle 50\n",
      "Done with angle 60\n",
      "Done with angle 70\n",
      "Done with angle 80\n",
      "Done with angle 90\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Based on dataset 3 of Dalila, taken from https://cernbox.cern.ch/index.php/s/KwFvdbub9QNP6qA Dataset3/HDF5/SiW\n",
    "    steps:\n",
    "        - load existing hdf5 files (1 per incident angle)\n",
    "        - transform shape (num_events, r_bins, alpha_bins, layer_id) to \n",
    "          (num_events, layer_id, alpha_bins, r_bins) as in dataset 1, then flatten last dimensions\n",
    "        - write to new hdf5 file\n",
    "\"\"\"\n",
    "\n",
    "folder = '../../../../ML_source/CaloChallenge/Dataset3/'\n",
    "angles = ['50', '60', '70', '80', '90']\n",
    "for angle in angles:\n",
    "    data_source = h5py.File(folder+'SiW_angle_{}.h5'.format(angle), 'r')\n",
    "    dataset_file = h5py.File(folder + 'dataset_3_{}.hdf5'.format(angle), 'w')\n",
    "\n",
    "    for n in range(0, 11):\n",
    "        energy = 2**n\n",
    "        data = data_source[str(2**n)]\n",
    "        data = np.moveaxis(data, 3, 1)\n",
    "        data = np.moveaxis(data, 3, 2)\n",
    "        dataset_file.create_dataset('data_'+str(energy*1000), data=data.clip(min=0.).reshape(len(data), -1), \n",
    "                                    compression='gzip')\n",
    "    data_source.close()\n",
    "    dataset_file.close()\n",
    "    print(\"Done with angle {}\".format(angle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa98af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing events in:\n",
    "# 50 2 (8000, 18, 50, 45)\n",
    "# 50 64 (8000, 18, 50, 45)\n",
    "# 70 256 (8000, 18, 50, 45)\n",
    "# 70 512 (8000, 18, 50, 45)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
